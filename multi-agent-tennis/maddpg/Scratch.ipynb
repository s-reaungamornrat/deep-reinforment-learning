{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from buffer import *\n",
    "from network import *\n",
    "from ou_noise import *\n",
    "\n",
    "class DDPG():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, device, state_size, action_size, random_seed, buffer_size, batch_size, \n",
    "             hidden_in_dim, hidden_out_dim, activation, gamma, tau, lr_actor, lr_critic,\n",
    "                 weight_decay, epsilon, epsilon_decay, num_batch_permute=10):\n",
    "             \n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(DDPG, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batch_permute =num_batch_permute\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=True).to(self.device)\n",
    "        self.actor_target = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=True).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=False).to(self.device)\n",
    "        self.critic_target = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=False).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "\n",
    "        # Same initialization\n",
    "        self.__copy__(self.actor_local, self.actor_target)\n",
    "        self.__copy__(self.critic_local, self.critic_target)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed, scale=1.0)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.device, action_size, buffer_size, self.batch_size, random_seed)\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, time):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size and time % 20 == 0:\n",
    "            for _ in range(self.num_batch_permute):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "                \n",
    "    def act(self, state, noise_scale=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float()\n",
    "            \n",
    "        action = self.actor_local(state.to(self.device)) + noise_scale*self.noise.noise()\n",
    "        return action\n",
    "\n",
    "    def target_act(self, state, noise_scale=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float()\n",
    "\n",
    "        action = self.actor_target(state.to(self.device)) + noise_scale*self.noise.noise()\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + ? * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.smooth_l1_loss(Q_expected, Q_targets.detach())\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)\n",
    "\n",
    "        # ---------------------------- update noise ---------------------------- #\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        self.noise.reset()\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        ?_target = t*?_local + (1 - t)*?_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def __copy__(self, source, target):\n",
    "        for src_param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(src_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ddpg import *\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, gamma, tau, ddpg_settings):\n",
    "    \n",
    "        '''\n",
    "            ddpg_settings: dict \n",
    "        '''\n",
    "        super(MADDPG, self).__init__()\n",
    "        self.marl = [DDPG(**ddpg_settings), DDPG(**ddpg_settings)]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "    \n",
    "    def act(self, obs_per_agent, noise_scale=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        actions = [agent.act(obs, noise_scale) for agent, obs in zip(self.marl, obs_per_agent)]\n",
    "        return actions\n",
    "    def target_act(self, obs_per_agent, noise_scale=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target_actions = [agent.target_act(obs, noise) for agent, obs in zip(self.marl, obs_per_agent)]\n",
    "        return target_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
