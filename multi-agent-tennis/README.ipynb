{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "The project is part of [Udacity Deep Reinforcement Learning Nano Degree program](!https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893), involving two agents playing tennis. The problem is solved using **Multi-Agent Deep Deterministic Policy Gradient** [MADDPG](!https://arxiv.org/abs/1706.02275) which allows multi-agents to either cooperate or compete or both in an environment.\n",
    "\n",
    "\n",
    "**Observation space** has 24 dimensions corresponding to the position and velocity of the ball and racket. Each agent receives its own local observation. <br>\n",
    "**Continuous action space** has 2 dimensions corresponding to jumping and movement toward (or away from) the net. <br>\n",
    "**Reward function**\n",
    "* +0.1 if an agent hits a ball over the net\n",
    "* -0.01 if an agent lets a ball hit the ground or hits the ball out of bounds <br>\n",
    "\n",
    "The problem is episodic with the goal of keeping the ball in play and considered solved if the agents attain an average score of +0.5 over 100 consecutive episodes. <br>\n",
    "\n",
    "<img src=\"images/tennis.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started\n",
    "\n",
    "### Conda Environment\n",
    "\n",
    "For those using `conda`, you can create a new `conda` environment as follows.\n",
    "* _Windows_\n",
    "```\n",
    "conda create --name your_env_name python=3.6 \n",
    "activate your_env_name \n",
    "```\n",
    "* _Linux_ or _MAC_\n",
    "```\n",
    "conda create --name your_env_name python=3.6 \n",
    "source activate your_env_name \n",
    "```\n",
    "\n",
    "### Project Dependencies\n",
    "\n",
    "The project requires the following libraries to be correctly installed. \n",
    "\n",
    "1. **Unity ML-Agents** <br>\n",
    "   The installation instruction can be found at [Unity-ML website](!https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n",
    "   * Download [Unity](!https://store.unity.com/download) \n",
    "   * Clone the ML-Agents Toolkit GitHub repository \n",
    "   ```\n",
    "   git clone https://github.com/Unity-Technologies/ml-agents.git\n",
    "   ```\n",
    "   * Download [requirements.txt](!https://github.com/Unity-Technologies/ml-agents/blob/master/python/requirements.txt) and install \n",
    "   ```\n",
    "   conda install --yes --file requirements.txt\n",
    "   ```\n",
    "2. **Pytorch** <br>\n",
    "   The pytorch version to install depends on your system configuration (e.g., operating systems, package managers, python versions, and availibility of CUDA). \n",
    "   * Select the configuration of your system at [pytorch website](!https://pytorch.org/) and follow the installation instruction described on the webpage.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructionsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The **MADDPG** agent described in `maddpg.py` comprises two **DDPG** defined in `ddpg.py`. Training the MADDPG agent can be done as described in Section 4 of `maddpg/Tennis.ipynb`. A snippet of the code to train an agent is shown below.\n",
    "```\n",
    "# determine torch computing device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set up environment\n",
    "env = UnityEnvironment(file_name=\"Banana.exe\")\n",
    "action_size = env.brains[env.brain_names[0]].vector_action_space_size  # get dimension of action space\n",
    "state_size = env.reset(train_mode=False)[env.brain_names[0]] .vector_observations.size # get dimension of state space\n",
    "\n",
    "# agent hyperparameters\n",
    "random_seed = 1234\n",
    "num_batch_permute = 10\n",
    "n_episodes=5000\n",
    "gamma = 0.99\n",
    "tau = 2.e-1#1.e-3\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 1000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ddpg_settings = {'state_size':state_size, 'action_size':action_size, 'random_seed':random_seed,\n",
    "                 'hidden_in_dim':128, 'hidden_out_dim':128, 'activation':F.relu,'tau':tau, \n",
    "                 'lr_actor':1e-3, 'lr_critic':1e-3, 'weight_decay':0., 'epsilon':1., 'epsilon_decay':0.}\n",
    "                 \n",
    "# create an agent and replay buffer\n",
    "maddpg = MADDPG(device, random_seed, gamma, ddpg_settings)\n",
    "replay_buffer = ReplayBuffer(device, action_size, buffer_size, batch_size, random_seed) \n",
    "\n",
    "# train the agent  \n",
    "scores = maddpg_training(maddpg, replay_buffer, n_episodes, noise_scale=2., noise_scale_reduction=0.9999, num_batch_permute=num_batch_permute)\n",
    "```\n",
    "`maddpg/TennisTesting.ipynb` shows how to load agent parameters and test the agent after training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
