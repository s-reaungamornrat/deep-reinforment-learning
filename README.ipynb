{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is part of **Udacity Deep Reinforcement Learning Nano Degree program**, involving training of an agent to collect bananas in a large square world. The agent is a [**Double-Deep Q Network (DDQN)**](!https://arxiv.org/abs/1509.06461) with [**Prioritized Experience Replay**](!https://arxiv.org/abs/1511.05952). As general reinforcement learning, the problem invloves environment states percieved by the agent, actions selected and performed by the agent, rewards returned by the environment for each action at the next time step, next states. The following describe the definition of each the components  \n",
    "- **Observation space** has 37 dimensions corresponding to the velocity of agent and ray-based perception of objects surrounding agent's forward direction\n",
    "- **Action space** has 4 dimensions corresponding to \n",
    "   - 0 - move forward\n",
    "   - 1 - move backward\n",
    "   - 2 - turn left\n",
    "   - 3 - turn right\n",
    "- **Reward function**\n",
    "   - +1 for collecting a yellow banana\n",
    "   - -1 for collecting a blue banana\n",
    "\n",
    "The problem is formed as an _episodic task_ with the goal of collecting as many bananas as possible. An episode ends when the maximum number of time steps is reached. The problem is considered **solved** if the agent attains **an average score of 13 over 100 consecutive episodes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conda Environment\n",
    "\n",
    "For those using `conda`, you can create a new `conda` environment as follows.\n",
    "* _Windows_\n",
    "```\n",
    "conda create --name your_env_name python=3.6 \n",
    "activate your_env_name \n",
    "```\n",
    "* _Linux_ or _MAC_\n",
    "```\n",
    "conda create --name your_env_name python=3.6 \n",
    "source activate your_env_name \n",
    "```\n",
    "\n",
    "### Project Dependencies\n",
    "\n",
    "The project requires the following libraries to be correctly installed. \n",
    "\n",
    "1. **Unity ML-Agents** <br>\n",
    "   The installation instruction can be found at [Unity-ML website](!https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)\n",
    "   * Download [Unity](!https://store.unity.com/download) \n",
    "   * Clone the ML-Agents Toolkit GitHub repository \n",
    "   ```\n",
    "   git clone https://github.com/Unity-Technologies/ml-agents.git\n",
    "   ```\n",
    "   * Download [requirements.txt](!https://github.com/Unity-Technologies/ml-agents/blob/master/python/requirements.txt) and install \n",
    "   ```\n",
    "   conda install --yes --file requirements.txt\n",
    "   ```\n",
    "2. **Pytorch** <br>\n",
    "   The pytorch version to install depends on your system configuration (e.g., operating systems, package managers, python versions, and availibility of CUDA). \n",
    "   * Select the configuration of your system at [pytorch website](!https://pytorch.org/) and follow the installation instruction described on the webpage.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DDQN** agent with **prioritized experience replay** is defined in Section 4 in `Navigation.ipynb`. Section 5 describes a method to train the agent. A snippet of the code to train an agent is shown below.\n",
    "```\n",
    "# determine torch computing device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set up environment\n",
    "env = UnityEnvironment(file_name=\"Banana.exe\")\n",
    "action_size = env.brains[env.brain_names[0]].vector_action_space_size  # get dimension of action space\n",
    "state_size = env.reset(train_mode=False)[env.brain_names[0]] .vector_observations.size # get dimension of state space\n",
    "\n",
    "# initilize an DDQN agent with prioritized experience replay\n",
    "agent = Agent(device, state_size, action_size)\n",
    "\n",
    "# train the agent  \n",
    "scores = dqn(env, agent, train_mode=True)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
