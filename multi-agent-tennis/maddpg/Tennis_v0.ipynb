{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('D:/UserData/Z003XD5A/dev/deep_rl_projects/ml-agents/python')\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tennis.exe', 'Tennis_Data', 'UnityPlayer.dll']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../../unity_environments/Tennis_Windows_x86_64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='../../unity_environments/Tennis_Windows_x86_64/Tennis.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "states shape  (2, 24)\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.69487906 -1.5\n",
      " -0.          0.          6.83172083  5.98822832 -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print('states shape ', states.shape)\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.10000000149011612\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore return states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states shape  (2, 24)\n",
      "scores  (2,)\n",
      "actions  (2, 2)\n",
      "next states  (2, 24)\n",
      "rewards type  <class 'list'>  len  2  rewards  [0.0, 0.0]\n",
      "dones type  <class 'list'>  len  2  any done  False  dones  [False, False]\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "print('states shape ', states.shape)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "print('scores ', scores.shape)\n",
    "actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "print('actions ', actions.shape)\n",
    "env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "print('next states ', next_states.shape)\n",
    "rewards = env_info.rewards                         # get reward (for each agent)\n",
    "print('rewards type ', type(rewards), ' len ', len(rewards), ' rewards ', rewards)\n",
    "dones = env_info.local_done                        # see if episode finished\n",
    "print('dones type ', type(dones), ' len ', len(dones), ' any done ', np.any(dones), ' dones ', dones)\n",
    "scores += env_info.rewards                         # update the score (for each agent)\n",
    "states = next_states                               # roll over states to next time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from maddpg import *\n",
    "#from ddpg import *\n",
    "from buffer import *\n",
    "#from network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_in_dim, hidden_out_dim, activation=F.relu, is_actor=False):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        \"\"\"self.input_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.input_norm.weight.data.fill_(1)\n",
    "        self.input_norm.bias.data.fill_(0)\"\"\"\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size,hidden_in_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_in_dim)\n",
    "        self.fc2_actor = nn.Linear(hidden_in_dim,hidden_out_dim)\n",
    "        self.fc2_critic = nn.Linear(hidden_in_dim+action_size,hidden_out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_out_dim)\n",
    "        self.fc3_actor = nn.Linear(hidden_out_dim,action_size)\n",
    "        self.fc3_critic = nn.Linear(hidden_out_dim,1)\n",
    "        self.activation = activation \n",
    "        self.is_actor = is_actor\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, x, action=None):\n",
    "        if self.is_actor:\n",
    "            # return a vector of the force\n",
    "            x = self.bn0(x)\n",
    "            x = self.activation(self.bn1(self.fc1(x)))\n",
    "            x = self.activation(self.bn2(self.fc2_actor(x)))\n",
    "            return torch.tanh(self.fc3_actor(x))\n",
    "        \n",
    "        else:\n",
    "            # critic network simply outputs a number\n",
    "            x = self.bn0(x)\n",
    "            x = self.activation(self.bn1(self.fc1(x)))\n",
    "            x = torch.cat((x, action), dim=-1)\n",
    "            x = self.activation(self.fc2_critic(x))\n",
    "            return self.fc3_critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from ou_noise import *\n",
    "\n",
    "class DDPG():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, device, state_size, action_size, random_seed, hidden_in_dim, hidden_out_dim, activation, \n",
    "                 tau, lr_actor, lr_critic, weight_decay, epsilon, epsilon_decay):\n",
    "             \n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(DDPG, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.device = device\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=True).to(self.device)\n",
    "        self.actor_target = Network(self.state_size, self.action_size, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=True).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Network(self.state_size*2, self.action_size*2, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=False).to(self.device)\n",
    "        self.critic_target = Network(self.state_size*2, self.action_size*2, hidden_in_dim, hidden_out_dim, activation=activation, is_actor=False).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "\n",
    "        # Same initialization\n",
    "        self.__copy__(self.actor_local, self.actor_target)\n",
    "        self.__copy__(self.critic_local, self.critic_target)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, seed=random_seed)\n",
    "                \n",
    "    def act(self, state, noise_scale=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float()\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state.to(self.device))\n",
    "        self.actor_local.train()\n",
    "        return action + noise_scale*self.noise.noise()\n",
    "\n",
    "    def target_act(self, state, noise_scale=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float()\n",
    "        return self.actor_target(state.to(self.device)) + noise_scale*self.noise.noise()\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def update_exploration_strategy(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + ? * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # ---------------------------- update noise ---------------------------- #\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        self.noise.reset()\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        ?_target = t*?_local + (1 - t)*?_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.-self.tau)*target_param.data)\n",
    "\n",
    "    def __copy__(self, source, target):\n",
    "        for src_param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(src_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, device, gamma, ddpg_settings):\n",
    "    \n",
    "        '''\n",
    "            ddpg_settings: dict \n",
    "        '''\n",
    "        super(MADDPG, self).__init__()\n",
    "        self.device = device\n",
    "        self.marl = [DDPG(device=device, **ddpg_settings), DDPG(device=device, **ddpg_settings)]\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def act(self, obs_per_agent, noise_scale=0.0):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\n",
    "            obs_per_agent: numpy of shape 2x24 where 2 is num agnets and 24 is state size\n",
    "        \"\"\"\n",
    "        # torch require input of shape [num_smaple, state_size] so we unsqueeze the 1st dim\n",
    "        actions = [agent.act(obs[np.newaxis,:], noise_scale) for agent, obs in zip(self.marl, obs_per_agent)]\n",
    "        return actions\n",
    "    \n",
    "    def target_act(self, obs_per_agent, noise_scale=0.0):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target_actions = [agent.target_act(obs[np.newaxis,:], noise) for agent, obs in zip(self.marl, obs_per_agent)]\n",
    "        return target_actions\n",
    "    \n",
    "    def critic_loss_function(self, agent_number, rewards, dones, next_local_states):\n",
    "        '''\n",
    "            agent : a selected ddpg agent\n",
    "        '''\n",
    "        \n",
    "        next_target_actions = [a.target_act(next_local_state) for a, next_local_state in zip(self.marl, next_local_states)]\n",
    "        #print('next_target_actions ', len(next_target_actions), next_target_actions[0].shape)\n",
    "        next_target_actions = torch.cat(next_target_actions, dim=-1)\n",
    "        #print('next_target_actions ', next_target_actions.shape)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.marl[agent_number].critic_target(next_global_states, next_target_actions.to(self.device))\n",
    "        #print('q_next ', q_next.shape)\n",
    "        #print('reward[agent_number] ', rewards[agent_number].shape, '  done[agent_number] ',  dones[agent_number].shape)\n",
    "        y = rewards[agent_number] + self.gamma * q_next * (1. - dones[agent_number])\n",
    "        #print('y ', y.shape)\n",
    "        #print('global states ', global_states.shape, ' actions ', actions.shape)\n",
    "        q = self.marl[agent_number].critic_local(global_states, actions)\n",
    "        #print('q ', q.shape)\n",
    "        return F.smooth_l1_loss(q, y.detach())\n",
    "    \n",
    "    def actor_loss_function(self, agent_number, local_states, global_states):\n",
    "        \n",
    "        predicted_actions = [self.marl[i].actor_local(state) if i == agent_number \\\n",
    "                             else self.marl[i].actor_local(state).detach() \\\n",
    "                             for i, state in enumerate(local_states)]\n",
    "        #print('predicted_actions ', len(predicted_actions), predicted_actions[0].shape)\n",
    "        predicted_actions = torch.cat(predicted_actions, dim=-1)\n",
    "        #print('predicted_actions ', predicted_actions.shape)\n",
    "        return -self.marl[agent_number].critic_local(global_states, predicted_actions).mean()\n",
    "    \n",
    "    def update(self, experiences, agent_number):\n",
    "        '''\n",
    "            Update learned critic and actor networks\n",
    "            experiences: random samples from replay buffer\n",
    "            agent_number: index to an agent\n",
    "        '''\n",
    "        local_states, global_states, actions, rewards, next_local_states, next_global_states, dones = experiences\n",
    "        # get selected agent\n",
    "        agent = self.marl[agent_number]\n",
    "        \n",
    "        #----------------update critic-----------------------\n",
    "        critic_loss = self.critic_loss_function(agent_number, rewards, dones, next_local_states)\n",
    "        agent.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(agent.critic_local.parameters(), 1.)\n",
    "        agent.critic_optimizer.step()\n",
    "        \n",
    "        #----------------update actor-----------------------\n",
    "        actor_loss = self.actor_loss_function(agent_number, local_states, global_states)\n",
    "        agent.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\n",
    "        agent.actor_optimizer.step()\n",
    "        \n",
    "        return critic_loss.cpu().detach().item(), actor_loss.cpu().detach().item()\n",
    "    \n",
    "    def update_targets(self):\n",
    "        '''Update target critic and actor networks'''\n",
    "        for agent in self.marl:\n",
    "            agent.soft_update(agent.critic_local, agent.critic_target)\n",
    "            agent.soft_update(agent.actor_local, agent.actor_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_transitions(transition):\n",
    "    print('obs ',len(transition), transition[0].shape)\n",
    "    print('obs_full ', len(transition), transition[1].shape)#, len(transition[1][0][0]))\n",
    "    print('actions ', len(transition),transition[2].shape)\n",
    "    print('rewards ', len(transition), transition[3].shape)#, len(transition[1][0][0]))\n",
    "    print('next obs ', len(transition), transition[4].shape)\n",
    "    print('next obs full ', len(transition), transition[5].shape)#, len(transition[1][0][0]))\n",
    "    print('dones ', len(transition), transition[6].shape)#, len(transition[1][0][0]))\n",
    "    #print(transition[6])\n",
    "def view_replay_buffer_data(replay_buffer):\n",
    "    if len(replay_buffer) > 0:\n",
    "        transition = replay_buffer.memory[0]\n",
    "    print_transitions(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "num_batch_permute = 10\n",
    "gamma = 0.99\n",
    "tau = 1.e-3\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ddpg_settings = {'state_size':state_size, 'action_size':action_size, 'random_seed':random_seed,\n",
    "            'hidden_in_dim':128, 'hidden_out_dim':128, 'activation':F.relu,\n",
    "            'tau':tau, 'lr_actor':1e-3, 'lr_critic':1e-3,\n",
    "           'weight_decay':0., 'epsilon':1., 'epsilon_decay':1e-6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = MADDPG(gamma, tau, ddpg_settings)\n",
    "replay_buffer = ReplayBuffer(device, action_size, buffer_size, batch_size, random_seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24)\n",
      "torch.Size([24]) tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -7.3793,  -1.5000,  -0.0000,   0.0000,   7.7616,   5.9882,\n",
      "         -0.0000,   0.0000,  -9.8775,  -0.9832, -24.9819,   6.2152,   7.7616,\n",
      "          5.8901, -24.9819,   6.2152])\n",
      "torch.Size([24]) tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -7.2174,  -1.5000,   0.0000,   0.0000,  -7.7616,   5.9882,\n",
      "          0.0000,   0.0000, -10.2174,  -1.5589, -30.0000,  -0.9810,  -7.7616,\n",
      "          5.8901, -30.0000,  -0.9810])\n",
      "act 0  torch.Size([1, 2]) tensor([[-0.1312,  0.5264]])\n",
      "actions  2  action0  torch.Size([1, 2])\n",
      "[tensor([[0.0553, 0.6361]]), tensor([[0.0751, 0.1770]])]\n",
      "(4,) \n",
      " [0.05529726 0.63613534 0.07509224 0.17695409]\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)\n",
    "state0 = torch.from_numpy(states[0]).float().to(device)\n",
    "state1 = torch.from_numpy(states[1]).float().to(device)\n",
    "print(state0.shape, state0)\n",
    "print(state1.shape, state1)\n",
    "act0 = maddpg.marl[0].act(state0.unsqueeze(0), 1.)\n",
    "print('act 0 ', act0.shape, act0)\n",
    "\n",
    "actions = maddpg.act(states, 1.)\n",
    "print('actions ', len(actions), ' action0 ', actions[0].shape)\n",
    "print(actions)\n",
    "actions = [action.numpy().squeeze() for action in actions] \n",
    "# print(actions)\n",
    "# actions+= [np.random.randn(1,2)]\n",
    "# print(actions)\n",
    "print(np.hstack(actions).shape, '\\n', np.hstack(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "obs  7 (2, 24)\n",
      "obs_full  7 (48,)\n",
      "actions  7 (4,)\n",
      "rewards  7 (2, 1)\n",
      "next obs  7 (2, 24)\n",
      "next obs full  7 (48,)\n",
      "dones  7 (2, 1)\n",
      "[[0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "max_t = 300\n",
    "noise_scale = 1.\n",
    "for i in range(6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    for i in range(max_t):\n",
    "        actions = maddpg.act(states, noise_scale)\n",
    "        #actions = np.vstack([action.numpy() for action in actions]) # num_agent x action_size\n",
    "        actions = np.hstack([action.numpy().squeeze() for action in actions])\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        replay_buffer.add(states, np.hstack(states), actions, np.vstack(rewards), next_states, np.hstack(next_states), np.vstack(dones).astype(np.uint8))\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "print(len(replay_buffer))\n",
    "view_replay_buffer_data(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  7 torch.Size([2, 256, 24])\n",
      "obs_full  7 torch.Size([256, 48])\n",
      "actions  7 torch.Size([256, 4])\n",
      "rewards  7 torch.Size([2, 256, 1])\n",
      "next obs  7 torch.Size([2, 256, 24])\n",
      "next obs full  7 torch.Size([256, 48])\n",
      "dones  7 torch.Size([2, 256, 1])\n"
     ]
    }
   ],
   "source": [
    "experiences = replay_buffer.sample()\n",
    "print_transitions(experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DDPG object at 0x000001E68A1C42E8>\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "agent_number = 0\n",
    "agent = maddpg.marl[agent_number]\n",
    "print(agent)\n",
    "print(maddpg.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#critic_target = Network(state_size*2, action_size*2, hidden_in_dim=128, hidden_out_dim=128, activation=F.relu, is_actor=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_states, global_states, actions, rewards, next_local_states, next_global_states, dones = experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_target_actions  2 torch.Size([256, 2])\n",
      "next_target_actions  torch.Size([256, 4])\n",
      "q_next  torch.Size([256, 1])\n",
      "reward[agent_number]  torch.Size([256, 1])   done[agent_number]  torch.Size([256, 1])\n",
      "y  torch.Size([256, 1])\n",
      "global states  torch.Size([256, 48])  actions  torch.Size([256, 4])\n",
      "q  torch.Size([256, 1])\n",
      "tensor(0.0015, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "next_target_actions = [agent.target_act(next_local_state, noise_scale) for agent, next_local_state in zip(maddpg.marl, next_local_states)]\n",
    "print('next_target_actions ', len(next_target_actions), next_target_actions[0].shape)\n",
    "next_target_actions = torch.cat(next_target_actions, dim=-1)\n",
    "print('next_target_actions ', next_target_actions.shape)\n",
    "with torch.no_grad():\n",
    "    q_next = agent.critic_target(next_global_states, next_target_actions.to(device))\n",
    "print('q_next ', q_next.shape)\n",
    "print('reward[agent_number] ', rewards[agent_number].shape, '  done[agent_number] ',  dones[agent_number].shape)\n",
    "y = rewards[agent_number] + maddpg.gamma * q_next * (1. - dones[agent_number])\n",
    "print('y ', y.shape)\n",
    "print('global states ', global_states.shape, ' actions ', actions.shape)\n",
    "q = agent.critic_local(global_states, actions)\n",
    "print('q ', q.shape)\n",
    "critic_loss = F.smooth_l1_loss(q, y.detach())\n",
    "print(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_actions  2 torch.Size([256, 2])\n",
      "predicted_actions  torch.Size([256, 4])\n",
      "tensor(0.0553, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "predicted_actions = [maddpg.marl[i].actor_local(state) if i == agent_number \\\n",
    "                     else maddpg.marl[i].actor_local(state).detach() \\\n",
    "                     for i, state in enumerate(local_states)]\n",
    "print('predicted_actions ', len(predicted_actions), predicted_actions[0].shape)\n",
    "predicted_actions = torch.cat(predicted_actions, dim=-1)\n",
    "print('predicted_actions ', predicted_actions.shape)\n",
    "actor_loss = -agent.critic_local(global_states, predicted_actions).mean()\n",
    "print(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128]) torch.Size([256, 4])\n",
      "torch.Size([256, 132])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "x = critic_target.bn0(next_global_states)\n",
    "x = critic_target.activation(critic_target.bn1(critic_target.fc1(x)))\n",
    "print(x.shape, next_target_actions.shape)\n",
    "x = torch.cat((x, next_target_actions), dim=-1)\n",
    "print(x.shape)\n",
    "x = critic_target.activation(critic_target.fc2_critic(x))\n",
    "print(x.shape)\n",
    "x = critic_target.fc3_critic(x)\n",
    "print(x.shape)\n",
    "x = critic_target(next_global_states, next_target_actions)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (2, 24)\n",
      "np_exp_local_states  (2, 256, 24)\n",
      "256 (48,)\n",
      "np_exp_global_states  (256, 48)\n",
      "256 (4,)\n",
      "np_exp_actions  (256, 4)\n",
      "256 (2, 1)\n",
      "np_exp_rewards  (2, 256, 1)\n",
      "256 (2, 24)\n",
      "np_exp_next_local_state  (2, 256, 24)\n",
      "256 (48,)\n",
      "np_exp_next_global_states  (256, 48)\n",
      "256 (2, 1)\n",
      "np_exp_dones  (2, 256, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[1],\n",
       "        [1]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8), array([[0],\n",
       "        [0]], dtype=uint8)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_local_states = [e.local_state for e in experiences if e is not None]\n",
    "print(len(exp_local_states), exp_local_states[0].shape)\n",
    "np_exp_local_states = np.stack(exp_local_states).swapaxes(0,1)\n",
    "print('np_exp_local_states ', np_exp_local_states.shape)\n",
    "### global state\n",
    "exp_global_states = [e.global_state for e in experiences if e is not None]\n",
    "print(len(exp_global_states), exp_global_states[0].shape)\n",
    "np_exp_global_states = np.stack(exp_global_states)\n",
    "print('np_exp_global_states ', np_exp_global_states.shape)\n",
    "### actions\n",
    "exp_actions = [e.action for e in experiences if e is not None]\n",
    "print(len(exp_actions), exp_actions[0].shape)\n",
    "np_exp_actions = np.stack(exp_actions)\n",
    "print('np_exp_actions ', np_exp_actions.shape)\n",
    "### rewards\n",
    "exp_rewards = [e.reward for e in experiences if e is not None]\n",
    "print(len(exp_rewards), exp_rewards[0].shape)\n",
    "np_exp_rewards = np.stack(exp_rewards).swapaxes(0,1)\n",
    "print('np_exp_rewards ', np_exp_rewards.shape)\n",
    "### next state\n",
    "exp_next_local_state = [e.next_local_state for e in experiences if e is not None]\n",
    "print(len(exp_next_local_state), exp_next_local_state[0].shape)\n",
    "np_exp_next_local_state = np.stack(exp_next_local_state).swapaxes(0,1)\n",
    "print('np_exp_next_local_state ', np_exp_next_local_state.shape)\n",
    "### next  global state\n",
    "exp_next_global_states = [e.next_global_state for e in experiences if e is not None]\n",
    "print(len(exp_next_global_states), exp_next_global_states[0].shape)\n",
    "np_exp_next_global_states = np.stack(exp_next_global_states)\n",
    "print('np_exp_next_global_states ', np_exp_next_global_states.shape)\n",
    "### dones\n",
    "exp_dones = [e.done for e in experiences if e is not None]\n",
    "print(len(exp_dones), exp_dones[0].shape)\n",
    "np_exp_dones = np.stack(exp_dones).swapaxes(0,1)\n",
    "print('np_exp_dones ', np_exp_dones.shape)\n",
    "exp_dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 1])\n"
     ]
    }
   ],
   "source": [
    "# shape [num_agents, num_samples, state_size] --- [2, batch_size, 24]\n",
    "local_states = torch.from_numpy(np.stack([e.local_state for e in experiences if e is not None]).swapaxes(0,1)).float().to(device)\n",
    "# shape [num_samples, num_agentsxstate_size] --- [batch_size, 2x24=48]\n",
    "global_states = torch.from_numpy(np.stack([e.global_state for e in experiences if e is not None])).float().to(device)\n",
    "# shape [num_samples, num_agentsxaction_size] --- [batch_size, 2x2=4]\n",
    "actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "# shape [num_agents, num_samples, rewards_size] --- [2, batch_size, 1]\n",
    "rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None]).swapaxes(0,1)).float().to(device)\n",
    "# shape [num_agents, num_samples, state_size] --- [2, batch_size, 24]\n",
    "next_local_states = torch.from_numpy(np.stack([e.next_local_state for e in experiences if e is not None]).swapaxes(0,1)).float().to(device)\n",
    "# shape [num_samples, num_agentsxstate_size] --- [batch_size, 2x24=48]\n",
    "next_global_states = torch.from_numpy(np.stack([e.next_global_state for e in experiences if e is not None])).float().to(device)\n",
    "# shape [num_agents, num_samples, done_size] --- [2, batch_size, 1]\n",
    "dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None]).swapaxes(0,1)).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
