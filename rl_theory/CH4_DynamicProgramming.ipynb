{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider that the **environment is a finite MDP** that is its state set $\\mathcal{S}$, action set $\\mathcal{A}$, and reward set $\\mathcal{R}$ are finite and its dynamics are given by a set of probabilities $p(s^\\prime, r\\:|\\:s, a)$, for all $s\\in\\mathcal{S}$, $a \\in \\mathcal{A}(s)$, $r\\in \\mathcal{R}$, and $s^\\prime \\in \\mathcal{S}^+$ ($\\mathcal{S}^+$ is $\\mathcal{S}$ plus a **terminal state** if the problem is episodic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Prediction)\n",
    "The problem of computing the state-value function $v_{\\pi}$ for an arbitrary policy $\\pi$ is called **policy evaluation** or the **prediction problem** in the DP literature. Recall from Chapter 3 that <br/>\n",
    "$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}\\:|\\:S_t = s]\\\\\n",
    "=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})\\:|\\:S_t=s ]\\\\\n",
    "=\\underset{a\\in\\mathcal{A}(s)}{\\sum}\\pi(a\\:|\\:s)\\underset{s^\\prime\\in\\mathcal{S}^+}{\\sum}\\underset{r\\in\\mathcal{R}}{\\sum}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma v_{\\pi}(s^\\prime)]$ <br/>\n",
    "The existence and uniquness of $v_{\\pi}$ are guaranteed as long as either $\\gamma<1$ or eventual termination is guaranteeed from all states under the policy $\\pi$. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dynamics of the environment is completely known, then $v_{\\pi}$ is solved as a solution of a system of nonlinear equations. However, **iterative solution methods** are most suitable practically. Consider a sequence of approximate value functions $v_{it}$: $v_0, v_1, v_2,...$, each mapping a state in $\\mathcal{S}^+$ to $\\mathbb{R}$. The initial approximation for $v_0$ can be chosen arbitrary, **except for the terminal state (if any) must be given value $0$**. The Bellman equation is used as an update rule for each successive approximation as <br/><br/>\n",
    "$v_{k+1}(s)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma v_k(S_{t+1})\\:|\\:S_t=s]\\\\\n",
    "=\\sum_{a\\in\\mathcal{A}(s)}\\pi(a\\:|\\:s)\\sum_{s^\\prime\\in\\mathcal{S}^+}\\sum_{r\\in\\mathcal{R}}p(s^\\prime, r\\:|\\:s,a)[r+\\gamma v_k(s^\\prime)]$ <br/><br/>\n",
    "Clearly, $v_k=v_{\\pi}$ is a fixed point for this update rule. Under the same conditions that guarantee the existence of $v_{\\pi}$, the sequence ${v_k}$ can be shown to converge to $v_{\\pi}$ as $k\\rightarrow\\infty$\n",
    "<br/>\n",
    "The algorithm using this update rule is called **iterative policy evaluation**. The update rule is called an **<span style=\"color:blue\">expected update</span>** since the update rule updates the value $v_{k+1}(s)$ of each state $s$ in the current iteration $k+1$ according to the value $v_k(s^\\prime)$ of its successor state $s^\\prime$ in the previous iteration $k$ and the expected immediate rewards, **along all the one-step transitions possible under the policy being evaluated**. There are several different kinds of expected updates, depending on whether a state or a state-action pair is being updated or the way the estimated values of the successors are combined. <span style=\"color:blue\">All the updates done in DP algorithms are called _expected_ update</span> since they are based on **<span style=\"color:blue\">an expectation over all possible next states</span>** rather than on a sample next state. The first backup diagram from Chapter 3 describes the nature of the expected update used in iterative policy evaluation. <img src=\"backup_diagram_v_pi.png\" alt=\"\" width=\"250\" height=\"250\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_py36]",
   "language": "python",
   "name": "conda-env-torch_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
