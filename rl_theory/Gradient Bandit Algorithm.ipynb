{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper-Confidence-Bound (UCB) Action Selection\n",
    "\n",
    "$A_t = \\underset{a}{\\operatorname{argmax}} [Q_t(a)+c\\sqrt{\\frac{\\ln t}{N_t(a)}}]$ <br/><br/>\n",
    "The square root term represents the uncertainty or variance in the estimate of $a$'s value. The term behind $\\operatorname{argmax}$ is thus a sort of upper bound on the possible true value of $a$ with $c$ determining the confidence level. <br/>\n",
    "Each time $a$ is selected the uncertainty is reduced: $N_t(a)$ increments. On the other hand, if an action other than $a$ is selected, $t$ increases but $N_t(a)$ does not; the uncertainty estimate increases. The use of $\\ln$ means the increases get smaller over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bandit Algorithm\n",
    "\n",
    "Update a preference $H_t(a)$ of an action $a$ instead of reward. $H_t(a)$ has no interpretation in terms of reward. Action probabilities are modelled by a soft-max distribution (i.e., Gib or Boltmann distribution): <br/>\n",
    "\n",
    "$Pr\\{A_t = a\\} = \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k} e^{H_t(b)} } = \\pi_t(a)$ represents the probability of taking action $a$ at time $t$.  <br/><br/>\n",
    "\n",
    "On each step, after selecting action $A_t$ and receiving the reward $R_t$, preferences are updated by: <br/>\n",
    "$H_{t+1}(A_t) = H_t(A_t)+\\alpha(R_t - Q_t)(1-\\pi_t(A_t))$ <br/>\n",
    "where $Q_t$ is the sample mean computed as in $\\epsilon$-greedy. For all $a \\ne A_t$, <br/>\n",
    "$H_{t+1}(a)=H_t(a)-\\alpha(R_t - Q_t)\\pi_t(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NonstationaryAction:\n",
    "    '''\n",
    "        Actions of which the expected rewards (true values) change over time.\n",
    "        In this case, it makes more sense to give more weight to recent rewards\n",
    "        than to long-past reqards, acheived by using a constant step-size within (0,1]\n",
    "        in the update rule.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, true_mean, initial_value = 0, step_size = 0.1, random_walk_std = 0.01):\n",
    "        '''\n",
    "            random_walk_std is a standard devitation of a Gaussian distribution\n",
    "            for modifying the true expected reward associated with each action\n",
    "        '''\n",
    "        # action value = expected reward\n",
    "        self.expected_mean = true_mean\n",
    "        # estimate of the action value = sample-mean of rewards\n",
    "        self.sample_mean = initial_value\n",
    "        # preference\n",
    "        self.preference = 0\n",
    "        self.is_selected = False\n",
    "        # number of times the action has been selected\n",
    "        self.number_of_times = 0 \n",
    "        # a constant step-size for the update rule\n",
    "        self.step_size = step_size\n",
    "        self.random_walk_std = random_walk_std\n",
    "        \n",
    "    \n",
    "    def copy(self, action):\n",
    "        \n",
    "        self.expected_mean = action.expected_mean\n",
    "        self.sample_mean = action.sample_mean\n",
    "        self.number_of_times = action.number_of_times\n",
    "        self.step_size = action.step_size\n",
    "        self.random_walk_std = action.random_walk_std\n",
    "        \n",
    "    def set_initial_value(self, initial_value):\n",
    "        self.sample_mean = initial_value\n",
    "        \n",
    "    def act(self):\n",
    "        return np.random.normal(self.expected_mean,self.random_walk_std,1)\n",
    "    \n",
    "    def nonstationary(self):\n",
    "        self.expected_mean += np.random.normal(0,1.,1)\n",
    "    \n",
    "    def update_value(self, reward):\n",
    "        '''\n",
    "            This function computes a sample mean of rewards after an action has been performed\n",
    "            i.e., recompute the sample mean given a new reward \n",
    "        '''\n",
    "        self.number_of_times += 1\n",
    "        # the updte rule\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonstationary_gradient_bandit(action_space, max_num_step = 100, epsilon = 0.1):\n",
    "    '''\n",
    "        This algorithm is for actions whose assocaited true expected rewards are nonstationary or vary from time step to time step\n",
    "        The algorithm selects an action having the highest value (sample-averaged reward) most of the times,\n",
    "        and with a probability of \\epsilon (probability of exploration) selects an action randomly with equal probability \n",
    "        (independence of its sample-averaged reward). \n",
    "        The typical value for \\epsilon is 0.05 or 0.1\n",
    "    '''\n",
    "    \n",
    "    def random_walk_action_value(action_space):\n",
    "        #print('Expected mean ', [a.expected_mean for a in action_space])\n",
    "        for a in action_space:\n",
    "            a.nonstationary()\n",
    "        #print('After changes, Expected mean ', [a.expected_mean for a in action_space])\n",
    "        \n",
    "    acquired_rewards = [] # acquired rewards at each time point\n",
    "\n",
    "    for n in range(max_num_step):\n",
    "        \n",
    "        prob = np.random.uniform(0.,1.,1)\n",
    "        \n",
    "        # simulate nonstationarity\n",
    "        random_walk_action_value(action_space)\n",
    "        \n",
    "        # select action according to probability of exploration\n",
    "        action_id = None\n",
    "        # explore other actions with probability of \\epsilon\n",
    "        if prob <= epsilon: \n",
    "            action_id = np.random.choice(len(action_space))\n",
    "        else: # pick the best action with probability 1-\\epsilon\n",
    "            est_mean_rewards = np.asarray([a.sample_mean for a in action_space])\n",
    "            action_id = np.argmax(est_mean_rewards)\n",
    "            \n",
    "        selected_action = action_space[action_id]\n",
    "        reward = selected_action.act()\n",
    "        selected_action.update_value(reward)\n",
    "        \n",
    "        # collect rewards for plotting\n",
    "        acquired_rewards.append(reward) \n",
    "        \n",
    "    # cumulative rewards at each time point\n",
    "    cumulative_rewards = np.cumsum(acquired_rewards)/(np.arange(max_num_step, dtype = np.float32)+1.0)\n",
    "    \n",
    "    return acquired_rewards, cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of actions\n",
    "nonstationary_action_space = []\n",
    "for i in range(20):\n",
    "    nonstationary_action_space.append(NonstationaryAction(np.random.uniform(0., 2., 1)))\n",
    "\n",
    "_, cumulative_rewards01 = nonstationary_bandit(nonstationary_action_space, 10000, 0.1)\n",
    "_, cumulative_rewards005 = nonstationary_bandit(nonstationary_action_space, 10000, 0.05)\n",
    "_, cumulative_rewards001 = nonstationary_bandit(nonstationary_action_space, 10000, 0.01)\n",
    "_, cumulative_rewards0 = nonstationary_bandit(nonstationary_action_space, 10000, 0.) # greedy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind-dl]",
   "language": "python",
   "name": "conda-env-aind-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
