{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Finite Markov Decision Processes\n",
    "\n",
    "A **finite** Markov decision process (MDP) is an MDP with **finite state, action, and reward sets**. Much of the current theory of reinforcement learning is restricted to finite MDPs, but the methods and ideas apply more generally.<br/><br/>\n",
    "In bandit problems we estimate the value $q_*(a)$ of each action $a$, in MDPs we estimate the value $q_*(s,a)$ of each action $a$ in each state $s$ or we estimate the value $v_*(s)$ of each state given **<span style=\"color:blue\">optimal action selections</span>**.\n",
    "We focus on assigning **<span style=\"color:blue\">credit for long-term consequences to individual action selections</span>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For particular values of the random variables $R_t$ and $S_t$, there is a probability of these values occuring at time $t$, given the values of the preceding state $S_{t-1} $and action $A_{t-1}$: <br/> \n",
    "$p(s^\\prime, r\\:|\\:s, a) =\\text{Pr}\\{S_t=s^\\prime, R_t=r| S_{t-1} = s, A_{t-1}=a\\}$, <br/> <br/>\n",
    "$p:\\mathcal{S}\\times\\mathcal{R}\\times\\mathcal{S}\\times\\mathcal{A}\\rightarrow [0,1]$ is a deterministic function defining the joint distribution of the states and rewards given the previous states and actions; thus <br/>\n",
    "$\\sum_{s^\\prime \\in \\mathcal{S}}\\sum_{r\\in \\mathcal{R}} p(s^\\prime,r\\: |\\: s,a)=1, \\forall s\\in\\mathcal{S}, a\\in\\mathcal{A} \\quad (3.3)$ <br/><br/>\n",
    "This joint distribution completely describes the whole dynamics of a **finite MDP**.From (3.3), we have **the state-transition probabilities** $p:\\mathcal{S}\\times\\mathcal{S}\\times\\mathcal{A}\\rightarrow[0,1]$  defined as <br/>\n",
    "$p(s^\\prime\\:|\\:s,a)=\\mbox{Pr}\\{S_t = s^\\prime \\:|\\: S_{t-1} = s, A_{t-1} = a\\}=\\sum_{r\\in\\mathcal{R}} p(s^\\prime, r\\:|\\:s,a) \\quad (3.4)$ <br/> <br/>\n",
    "From (3.3), the **expected rewards** for state-action pairs is defined as a 2-argument function, $r:\\mathcal{S}\\times\\mathcal{A}\\rightarrow \\mathbb{R}$:\n",
    "$r(s,a) = \\mathbb{E}[R_t \\:|\\: S_{t-1} = s, A_{t-1} = a]=\\sum_{r\\in \\mathcal{R}}r\\sum_{s^\\prime}p(s^\\prime,r\\:|\\:s,a)  \\quad (3.5)$ <br/> <br/>\n",
    "The expected rewards as a 3-argument function is defined as $r:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$<br/>\n",
    "$r(s^\\prime,s,a)=\\mathbb{E}[R_t \\:|\\: S_{t}=s^\\prime, A_t = a, S_{t-1}=s] \\\\\n",
    "=\\sum_{r\\in\\mathcal{R}} r \\sum_{s^\\prime\\in\\mathcal{S}} p(r\\:|\\: s^\\prime, a, s) \\\\\n",
    "= \\sum_{r\\in\\mathcal{R}} r \\sum_{s^\\prime\\in\\mathcal{S}} \\frac{p(s^\\prime, r\\:|\\: a, s)}{p( s^\\prime\\:|\\:s,a)} \\quad (3.6) $ <br/> \n",
    "where $p(r\\:|\\: s^\\prime, a, s) = \\frac{p(s^\\prime, r\\:|\\: s,a)}{p( s^\\prime\\:|\\:s,a)}$ from the definition of a conditional probability <br/>\n",
    "$p(a\\:|\\:b) = \\frac{p(a,b)}{p(b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Return \n",
    "The objective/goal of the agent is to maximize the cumulative reward it receives in the long run. If the sequence of rewards recieved after time step $t$ is denoted as $R_{t+1}, R_{t+2}, R_{t+3}, ...$, then the **expected return** at time step $t$ is <br/>\n",
    "$G_t=R_{t+1}+R_{t+2}+R_{t+3}+...+R_T$ <br/>\n",
    "where $T$ is a final time step. In other words, the **return** is the function of future rewards.<br/><br/>\n",
    "For a continuing task, we compute **expected discounted return** using a **discount rate** $\\gamma\\in[0,1]$ as <br/>\n",
    "$G_t = R_{t+1}+\\gamma R_{t+2} +\\gamma^2 R_{t+3}+...\\\\\n",
    "= \\sum_{i=1}^{\\infty} \\gamma^{i-1}R_{t+i} = \\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1}$ <br/><br/>\n",
    "\n",
    "We can rewrite the expected discounted return using recursion as: <br/>\n",
    "$G_t=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}+.... =R_{t+1}+ \\gamma G_{t+1}$ <br/>\n",
    "since $G_{t+1}=R_{t+2}+\\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\gamma^3 R_{t+5}+....$. <br/> <br/>\n",
    "\n",
    "This works for all time step $t <T$ and if we define $G_T = 0$, it works when termination occurs. We can also define the expected discounted return as <br/>\n",
    "$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1}R_k$ <br/>\n",
    "with possibility that $T=\\infty$ or $\\gamma = 1$ but not both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "A policy is a mapping from states to probabilities of selecting each possible action for each time step $t$ <br/>\n",
    "$\\pi(a|s):\\mathcal{S}\\rightarrow\\mbox{Pr}\\{A_t=a\\:|\\:S_t=a\\}$ <br/>\n",
    "It defines **a probability over $a\\in\\mathcal{A}$ for each $s\\in\\mathcal{S}$**. <br/><br/>\n",
    "The value of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the **expected return** when starting from $s$ and following $\\pi$ thereafter. <br/><br/>\n",
    "For MDPs, the value of state called the **state-value function for policy $\\pi$** is <br/>\n",
    "$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t\\:|\\: S_t=s]=\\mathbb{E}_{\\pi}[\\sum_{i=0}^{\\infty}\\gamma^iR_{t+i+1}\\:|\\: S_t=s]$ <br/>where $t$ is any time step.<br/>\n",
    "**The value of the terminal state is always 0**. <br/><br/>\n",
    "The value of taking an action $a$ in a state $s$ under a policy $\\pi$, called the **action-value function for policy $\\pi$**, is the expected return starting from $s$, taking the action $a$, and thereafter following the policy $\\pi$ <br/>\n",
    "$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\:|\\:S_t = s, A_t = a]$ <br/><br/>\n",
    "\n",
    "The important property of the state-value function and the action-value function (i.e., value functions) is that they satisfy recursive relations. <br/>\n",
    "$v_{\\pi}(s) = \\mathbb{E}[G_t\\:|\\:S_t=s] \\\\\n",
    "=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}\\:|\\:S_t=s] \\\\\n",
    "=\\sum_{s^\\prime\\in\\mathcal{S}}^{}\\sum_{r\\in\\mathcal{R}}^{}\\sum_{a\\in\\mathcal{A}(s)}^{}p(s^\\prime, r\\:|\\:s,a)\\pi(a|s)[r+\\gamma v_{\\pi}(s^\\prime)] \\quad (3.14)$ <br/>\n",
    "This equation is also known as the **self-consistency condition.**\n",
    "Note that $a$ is taken from $\\mathcal{A}(s)$. The final expression can be read easily as an expected value with the probability $p(s^\\prime, r\\:|\\:s,a)\\pi(a|s)$ weighting $[r+\\gamma v_{\\pi}(s^\\prime)]$. <br/><br/>\n",
    "Equation (3.14) is the **Bellman equation for $v_{\\pi}$**. <img src=\"backup_diagram_v_pi.png\" alt=\"\" width=\"250\" height=\"250\">\n",
    "\n",
    "The backup diagram for the **Bellman equation for $q_{\\pi}$** is <br/> <img src=\"backup_diagram_q_pi.png\" alt=\"\" width=\"250\" height=\"2500\">\n",
    "\n",
    "and the **Bellman equation for $q_{\\pi}$** is <br/>\n",
    "$q_{\\pi}(s,a) = \\mathbb{E}[G_t \\:|\\: S_t=s, A_t = a] \\\\\n",
    "=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1} \\:|\\: S_t=s, A_t = a] \\\\\n",
    "=\\sum_{r\\in\\mathcal{R}}\\sum_{s^\\prime\\in \\mathcal{S}}p(s^\\prime, r\\:|\\:s,a)[r+\\gamma\\sum_{a^\\prime\\in \\mathcal{A}(s^\\prime)}\\pi(a^\\prime\\:|\\:s^\\prime)q_{\\pi}(s^\\prime, a^\\prime)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the backup diagram <img src=\"v_pi_func_of_q_pi.png\" alt=\"Backup diagram for an state-value function\" width=\"700\" height=\"800\">The value of a state $v_{\\pi}(s)$ in terms of $q_{\\pi}(s,a)$ is <br/>\n",
    "$v_{\\pi}(s) = \\mathbb{E}[R_{t+1}+\\gamma G_{t+1} \\:|\\: S_t = s] \\\\\n",
    "=\\sum_{a\\in\\mathcal{A}}\\pi(a\\:|\\:s)q_{\\pi}(s,a)$ <br/><br/>\n",
    "On the other hand, the value of an action $q_{\\pi}(s,a)$ depends on **the expected next reward** and **the expected sum of the remaining rewards**. Its corresponding small backup diagram is <img src=\"q_pi_func_of_v_pi.png\" alt=\"Backup diagram for an action-value function\" width=\"500\" height=\"600\">and the function is <br/>\n",
    "$q_{\\pi}(s,a) = \\mathbb{E}[R_{t+1}+\\gamma G_{t+1} \\:|\\: S_t = s, A_t = a] \\\\\n",
    "=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\:|\\: S_t = s, A_t = a] \\\\\n",
    "=\\sum_{s^\\prime\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s^\\prime, r\\:|\\:s,a)[r+\\gamma v_{\\pi}(s^\\prime)]$ <br/>\n",
    "<br/>\n",
    "If we substitute this $q_{\\pi}(s,a)$ in the above $v_{\\pi}(s)$, we obtain <br>\n",
    "$v_{\\pi}(s) =\\sum_{a\\in\\mathcal{A}}\\pi(a\\:|\\:s)[\\sum_{s^\\prime\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s^\\prime, r\\:|\\:s,a)[r+\\gamma v_{\\pi}(s^\\prime)]]$ <br/>\n",
    "we get the exact same equation as in (3.14). <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each reinforcement learning problem, there may be more than optimal policy. We denote **all optimal policies** by $\\pi_*$. All the optimal oplicies $\\pi_*$ share the same state-value function, called the **optimal state-value** function, <br/>\n",
    "$v_*(s) = \\max_{\\pi} v_{\\pi}(s)$ for all $s \\in \\mathcal{S}$<br/>\n",
    "$\\pi_*$ also share the same action-value function, called the **optimal action-value** function,  <br/>\n",
    "$q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$ for all $s \\in \\mathcal{s}$ and $a \\in \\mathcal{A}(s)$<br/>\n",
    "Recall that $q_*(s,a)$ gives the expected return for taking an action $a$ in $s$ and thereafter following $\\pi_*$. Thus, we can write $q_*(s,a)$ in terms of $v_*(s)$ as <br/>\n",
    "$q_*(s,a) = \\mathbb{E}[R_{t+1}+\\gamma v_*(S_{t+1})\\: | \\:S_t = s, A_t = a]$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **consistency condition** (3.14) for the optimal value functions $v_*$ can be written in a special form without reference to any specific policy as <br/>\n",
    "$v_*(s) = \\max_{a\\in\\mathcal{A}(a)}q_{\\pi^*}(s,a)\\\\\n",
    "= \\max_{a\\in\\mathcal{A}(a)}\\mathbb{E}_{\\pi^*}[R_{t+1}+\\gamma G_{t+1} \\:|\\: S_t = s, A_t = a]\\\\\n",
    " = \\max_{a\\in\\mathcal{A}(a)}\\mathbb{E}[R_{t+1}+\\gamma v_*(S_{t+1}) \\:|\\: S_t = s, A_t = a]\\\\\n",
    "=\\max_{a\\in\\mathcal{A}(a)}\\sum_{s^\\prime}\\sum_{r}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma v_{*}(s^\\prime)]\n",
    "$\n",
    "\n",
    "This is the **Bellman optimiality equation** expressing the fact that **the value of a state under the optimal policy is the expected return for the best action from that state**.\n",
    "The **Bellman optimality equation** for $q_{\\pi}$ is <br/>\n",
    "$q_*(s,a) =\\mathbb{E}[R_{t+1}+\\gamma \\max_{a^\\prime\\in\\mathcal{A}(S_{t+1})} q_*(S_{t+1},a^\\prime)\\:|\\:S_t=s, A_t = a] \\\\\n",
    "= \\sum_{r\\in\\mathcal{R}}\\sum_{s^\\prime\\in\\mathcal{S}}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma\\max_{a^\\prime\\in\\mathcal{A}(s^\\prime)} q_*(s^\\prime,a^\\prime)]$  <br/>\n",
    "The backup diagrams for both Bellman optimality equations are shown below <img src=\"bellman_optimality.png\" alt=\"\" width=\"500\" height=\"600\">. The arcs at the agent's choice points represent the fact that the maximum over choices is taken rather than the expected value given some policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finite MDPs (i.e., finite number of states and actions), the Bellman optimaility equation for $v_{\\pi}$ has a **unique solution independent of the policy**. The Bellman optimality equation is a system of equations, one for each state, so if there are $n$ states, then there are $n$ equations in $n$ unknowns. If the dynamics $p$ of the environment are known, in principle one can solve this system for $v_*$ using methods for solving systems of nonlinear equations. Also can solve a related set of equations for $q_*$. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.23** Give an equation for $v_*$ in terms of $q_*$ <br/>\n",
    "$v_*(s) = \\max_{a\\in\\mathcal{A}(s)}q_*(s,a)$ <br/><br/>\n",
    "**Exercise 3.24** Give an equation for $q_*$ in terms of $v_*$ and the world's dynamics, $p(s^\\prime,r\\:|\\:s,a)$. <br/>\n",
    "$q_*(s,a)=\\sum_{s^\\prime\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma \\max_{a^\\prime\\in\\mathcal{A}(s^\\prime)}q_*(s^\\prime, a^\\prime)]\\\\\n",
    "=\\sum_{s^\\prime\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma v_*(s^\\prime)]$ <br/><br/>\n",
    "**Exercise 3.25** Give an equation for $\\pi_*$ in terms of $q_*$. <br/>\n",
    "$ \\pi_*(a\\:|\\:s)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ a=\\underset{a\\in\\mathcal{A}(s)}{\\operatorname{argmax}} q_*(s,a) \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}$\n",
    "<br/><br/>\n",
    "**Exercise 3.26** Give an equation for $\\pi_*$ in terms of $v_*$ and the world's dynamics, $p(s^\\prime, r\\:|\\:s,a)$. <br/>\n",
    "$ \\pi_*(a\\:|\\:s)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ a=\\underset{a\\in\\mathcal{A}(s)}{\\operatorname{argmax}} \\sum_{s^\\prime\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}p(s^\\prime,r\\:|\\:s,a)[r+\\gamma v_*(s^\\prime)] \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}$\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* For a finite MDP, even though the optimal value functions for states and state-action pairs are unique, there can be many **optimal policies**.\n",
    "* Any policy that is greedy with respect to the optimal value functions must be an optimal policy.\n",
    "* A reinforncement learning problem can be categorized in a variety of different ways, for example according to the level knowledge initially available to the agent.\n",
    "  * Complete knowledge: the agent has a complete and accurate model of the environment's dynamics. If the environment is an MDP, then the model consists of the complete four-argument dynamics function $p(s^\\prime,r\\:|\\:s,a)$.\n",
    "  * Incomplete knowledge: a perfect model of the environment is not available\n",
    "* Computational limits (runtime and memory) typically prevents the agent from fully using the complete knowledge, if it has."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind-dl]",
   "language": "python",
   "name": "conda-env-aind-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
